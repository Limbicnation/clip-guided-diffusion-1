{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CLIP Guided Diffusion \n",
    "\n",
    "**From [RiversHaveWings](https://twitter.com/RiversHaveWing)**\n",
    "\n",
    "Generate vibrant and detailed images using only text.\n",
    "- Originally by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). \n",
    "- Python repository and additions assembled by Clay Mullis (https://github.com/afiaka87)\n",
    "\n",
    "**[Read Me](https://github.com/afiaka87/clip-guided-diffusion)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Licensed under the MIT License\n",
    "\n",
    "# Copyright (c) 2021 Clay Mullis\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "# THE SOFTWARE."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Grab code and install requirements\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Install custom fork of guided-diffusion with init image/class scoring support\n",
    "%pip -q install 'git+https://github.com/afiaka87/guided-diffusion@main'\n",
    "\n",
    "colab_dir = Path(\"/content\")\n",
    "if 'clip-guided-diffusion' not in os.getcwd():\n",
    "    os.chdir(colab_dir)\n",
    "    if not Path('/content/clip-guided-diffusion').exists():\n",
    "        !git clone https://github.com/afiaka87/clip-guided-diffusion.git\n",
    "        %pip install -r /content/clip-guided-diffusion/requirements.txt\n",
    "    else:\n",
    "        print(\"Clip-guided-diffusion already installed\")\n",
    "else:\n",
    "    print(\"localhost detected\")\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run\n",
    "\n",
    "Generate an image using CLIP guided diffusion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@title Run\n",
    "##############################\n",
    "# Imports\n",
    "##############################\n",
    "import sys\n",
    "\n",
    "if root_path is None:\n",
    "    sys.path.append(f\"{root_path}/clip-guided-diffusion/\")\n",
    "    sys.path.append(f\"{root_path}/clip-guided-diffusion/guided-diffusion\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import kornia.augmentation as kaugs\n",
    "import torch as th\n",
    "from tqdm.auto import tqdm\n",
    "from IPython import display as ipy_display\n",
    "\n",
    "from cgd import clip_guided_diffusion\n",
    "import cgd_util\n",
    "\n",
    "##############################\n",
    "# Config\n",
    "##############################\n",
    "\n",
    "prompt = \"An image of a mushroom\"  #@param{type: \"string\"}\n",
    "\n",
    "prompt_min = \"the color green\"  #@param{type: \"string\"}\n",
    "prompt_min = None if len(prompt_min) == 0 else prompt_min\n",
    "min_weight = 0.1  #@param{type: \"number\"}\n",
    "class_score = False\n",
    "\n",
    "output_dir = \"outputs\"  #@param {type: \"string\"}\n",
    "output_dir = Path(output_dir)\n",
    "\n",
    "#@markdown `timestep_respacing` - Number of timesteps to visit out of total `diffusion_steps`.\n",
    "#@markdown  - Smaller values are much faster but less accurate.\n",
    "#@markdown  - Larger values are very accurate but take much longer.\n",
    "timestep_respacing = \"250\"  #@param{type:\"string\"}\n",
    "tv_scale = 100 #@param{type: \"number\"}\n",
    "\n",
    "#@markdown `init_image` - Sample image for `skip_timsteps` before guiding with CLIP.\n",
    "#@markdown `skip_timesteps` should be less than `timestep_respacing` and `diffusion_steps`\n",
    "skip_timesteps = 0  #@param{type:\"number\"}\n",
    "init_image = \"\"  #@param{type: \"string\"}\n",
    "init_image = None if len(init_image) == 0 else init_image\n",
    "\n",
    "checkpoints_dir = Path(\"checkpoints\")\n",
    "checkpoints_dir.mkdir(exist_ok=True)\n",
    "\n",
    "save_frequency = 25  #@param {type: \"number\"}\n",
    "\n",
    "batch_size = 1  #@param{type: \"number\"}\n",
    "top_n = 250  #@param{type: \"number\"}\n",
    "image_size = 256  #@param{type: \"number\"}\n",
    "\n",
    "#@markdown disabling class_cond will also disable class randomization/clip scoring\n",
    "class_cond = True  #@param{type: \"boolean\"}\n",
    "clip_guidance_scale = 1000  #@param {type: \"number\"}\n",
    "\n",
    "#@markdown cutout power can be decreased with sufficiently high num_cutouts\n",
    "cutout_power = 0.5  #@param{type: \"number\"}\n",
    "num_cutouts = 16  #@param{type: \"number\"}\n",
    "seed = 0  #@param{type: \"number\"}\n",
    "diffusion_steps = 1000  #@param{type:\"number\"}\n",
    "clip_model_name = \"ViT-B/32\"  #@param{type: \"string\"}\n",
    "\n",
    "# I have not experimented with transforms much; these are merely here to showcase how to use them.\n",
    "random_affine = False #@param{type: \"boolean\"}\n",
    "random_motion_blur = False #@param{type: \"boolean\"}\n",
    "random_horizontal_flip = False #@param{type: \"boolean\"}\n",
    "\n",
    "augs = []\n",
    "if random_affine:\n",
    "    augs.append(kaugs.RandomAffine(degrees=0, translate=(\n",
    "        0.1, 0.1), scale=(0.9, 1.1), shear=0.1))\n",
    "if random_motion_blur:\n",
    "    augs.append(kaugs.RandomMotionBlur(\n",
    "        kernel_size=(1, 5), angle=15, direction=0.5))\n",
    "if random_horizontal_flip:\n",
    "    augs.append(kaugs.RandomHorizontalFlip(p=0.5))\n",
    "\n",
    "##############################\n",
    "# Actually do the run...\n",
    "##############################\n",
    "clear_scrollback = False  #@param {type: \"string\"}\n",
    "\n",
    "assert 0 < save_frequency <= int(timestep_respacing.replace('ddim', '')), \\\n",
    "    \"--save_frequency/--freq must be greater than 0and less than --timestep_respacing\"\n",
    "\n",
    "# convert Path arg to Path object\n",
    "prefix_path = Path('./outputs')\n",
    "prefix_path.mkdir(exist_ok=True)\n",
    "assert prefix_path.is_dir(\n",
    "), f\"--prefix,-dir {prefix_path} is a file, not a directory. Please provide a directory.\"\n",
    "\n",
    "# Initialize diffusion generator\n",
    "cgd_samples, _, diffusion = clip_guided_diffusion(\n",
    "    prompt=prompt, prompt_min=prompt_min, min_weight=min_weight, batch_size=batch_size, tv_scale=tv_scale, top_n=top_n,\n",
    "    image_size=image_size, class_cond=class_cond, clip_guidance_scale=clip_guidance_scale, cutout_power=cutout_power,\n",
    "    num_cutouts=num_cutouts, timestep_respacing=timestep_respacing, custom_device='cuda' if th.cuda.is_available() else 'cpu',\n",
    "    seed=seed, diffusion_steps=diffusion_steps, skip_timesteps=skip_timesteps, init_image=init_image, checkpoints_dir=checkpoints_dir,\n",
    "    clip_model_name=clip_model_name, class_score=class_score,\n",
    ")\n",
    "\n",
    "\n",
    "# Remove non-alphanumeric and white space characters from prompt and prompt_min for directory name\n",
    "outputs_path = cgd_util.txt_to_dir(base_path=prefix_path, txt=prompt, txt_min=prompt_min)\n",
    "outputs_path.mkdir(exist_ok=True)\n",
    "\n",
    "all_images = []\n",
    "try:\n",
    "    current_timestep = diffusion.num_timesteps - 1\n",
    "    for step, sample in enumerate(cgd_samples):\n",
    "        current_timestep -= 1\n",
    "        if step % save_frequency == 0 or current_timestep == -1:\n",
    "            for j, image in enumerate(sample[\"pred_xstart\"]):\n",
    "                image_path = Path(cgd_util.log_image(image, prefix_path, step, j))\n",
    "                ipy_display.display(ipy_display.Image(\"current.png\"))\n",
    "                all_images.append(image_path)\n",
    "            if clear_scrollback:\n",
    "                ipy_display.clear_output()\n",
    "except RuntimeError as runtime_ex:\n",
    "    if \"CUDA out of memory\" in str(runtime_ex):\n",
    "        tqdm.write(f\"CUDA OOM error occurred.\")\n",
    "        tqdm.write(f\"Try lowering --image_size/-size, --batch_size/-bs, --num_cutouts/-cutn\")\n",
    "        tqdm.write(f\"--clip_model/-clip (currently '{clip_model_name}') can have a large impact on VRAM usage.\")\n",
    "        tqdm.write(f\"RN50 will use the least VRAM. ViT-B/32 is the best bang for your buck.\")\n",
    "        tqdm.write(f\"(colab) Restart the runtime. This is required upon rerunning cells due to a memory leak.\")\n",
    "        # I have tried everything to get rid of this error, but it seems to be a memory leak\n",
    "        # At this point; you should probably just restart the runtime.\n",
    "        import gc\n",
    "        th.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        from cgd import clip_guided_diffusion\n",
    "    else:\n",
    "        raise runtime_ex\n",
    "\n",
    "print(f\"Saved {len(all_images)} total images from guided diffusion to {outputs_path})\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('cgd_venv': venv)"
  },
  "interpreter": {
   "hash": "df369922dce60aef3e878b1b53550d3a55b368f1ac0931e547f318c968180cb0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}