{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.9.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.1 64-bit ('cgd_venv': venv)"
    },
    "interpreter": {
      "hash": "df369922dce60aef3e878b1b53550d3a55b368f1ac0931e547f318c968180cb0"
    },
    "colab": {
      "name": "cgd.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/afiaka87/clip-guided-diffusion/blob/main/cgd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP Guided Diffusion \n",
        "\n",
        "**From [RiversHaveWings](https://twitter.com/RiversHaveWing)**\n",
        "\n",
        "Generate vibrant and detailed images using only text.\n",
        "- Originally by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). \n",
        "- Python repository and additions assembled by Clay Mullis (https://github.com/afiaka87)\n",
        "\n",
        "**[Read Me](https://github.com/afiaka87/clip-guided-diffusion)**"
      ],
      "metadata": {
        "id": "f1Tay1Y169ru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Clay Mullis\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "outputs": [],
      "metadata": {
        "id": "SsoBNFpL69rv",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Grab code and install requirements\n",
        "# colab only\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "%cd /content\n",
        "\n",
        "!git clone 'https://github.com/afiaka87/clip-guided-diffusion'\n",
        "!git clone 'https://github.com/afiaka87/guided-diffusion'\n",
        "%pip install -r /content/clip-guided-diffusion/requirements.txt \n",
        "%pip install -e guided-diffusion\n",
        "\n",
        "sys.path.append(\"/content/guided-diffusion\")\n",
        "sys.path.append(\"/content/clip-guided-diffusion\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "XnF35z2h69rw",
        "outputId": "19b1c4e6-38e0-4ce2-e4e7-825e6a3c99e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "##############################\n",
        "# Imports\n",
        "##############################\n",
        "import sys\n",
        "sys.path.append(\"guided-diffusion\")\n",
        "sys.path.append(\"clip-guided-diffusion\" if \"clip-guided-diffusion\" not in os.getcwd() else \".\")\n",
        "from pathlib import Path\n",
        "\n",
        "import kornia.augmentation as kaugs\n",
        "import torch as th\n",
        "from tqdm.auto import tqdm\n",
        "from IPython import display as ipy_display\n",
        "\n",
        "from cgd import clip_guided_diffusion\n",
        "import cgd_util\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "dfcH769Y69rx",
        "outputId": "4c93810d-1763-471d-9816-8f22779247e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "#@title Config\n",
        "##############################\n",
        "# Config\n",
        "##############################\n",
        "\n",
        "prompt = \"An image of a mushroom\"  #@param{type: \"string\"}\n",
        "\n",
        "prompt_min = \"the color green\"  #@param{type: \"string\"}\n",
        "prompt_min = None if len(prompt_min) == 0 else prompt_min\n",
        "min_weight = 0.1  #@param{type: \"number\"}\n",
        "\n",
        "tv_scale = 100  #@param{type: \"number\"}\n",
        "\n",
        "#@markdown `class_score` Rank classes with CLIP instead of uniform random selection\n",
        "class_score = True  #@param{type: \"boolean\"}\n",
        "#@markdown `top_n` max classes to rank.\n",
        "top_n = 1000  #@param{type: \"number\"}\n",
        "\n",
        "output_dir = \"outputs\"  #@param {type: \"string\"}\n",
        "output_dir = Path(output_dir)\n",
        "\n",
        "save_frequency = 25  #@param {type: \"number\"}\n",
        "\n",
        "batch_size = 1  #@param{type: \"number\"}\n",
        "image_size = \"128\"  #@param [\"64\", \"128\", \"256\", \"512\"]\n",
        "image_size = int(image_size)\n",
        "\n",
        "#@markdown `timestep_respacing` - Number of timesteps to visit out of total `diffusion_steps`.\n",
        "#@markdown  - Smaller values are much faster but less accurate.\n",
        "#@markdown  - Larger values are very accurate but take much longer.\n",
        "#@param [\"25\", \"50\", \"100\", \"250\", \"500\", \"1000\", \"ddim25\", \"ddim50\", \"ddim100\", \"ddim250\", \"ddim500\", \"ddim1000\"] {allow-input: true}\n",
        "timestep_respacing = \"250\"\n",
        "\n",
        "#@markdown `init_image` - Sample image for `skip_timsteps` before guiding with CLIP.\n",
        "#@markdown `skip_timesteps` should be less than `timestep_respacing` and `diffusion_steps`\n",
        "skip_timesteps = 0  #@param{type:\"number\"}\n",
        "init_image = \"\"  #@param{type: \"string\"}\n",
        "init_image = None if len(init_image) == 0 else init_image\n",
        "\n",
        "checkpoints_dir = Path(\"checkpoints\")\n",
        "checkpoints_dir.mkdir(exist_ok=True)\n",
        "\n",
        "#@markdown disabling `class_cond` will also disable class randomization/clip scoring\n",
        "class_cond = True  #@param{type: \"boolean\"}\n",
        "clip_guidance_scale = 1000  #@param {type: \"number\"}\n",
        "\n",
        "#@markdown `cutout_power` can be decreased to 0.5 with sufficiently high num_cutouts.\n",
        "cutout_power = 1.0  #@param{type: \"number\"}\n",
        "num_cutouts = 16  #@param{type: \"number\"}\n",
        "seed = 0  #@param{type: \"number\"}\n",
        "diffusion_steps = 1000  #@param{type:\"number\"}\n",
        "#@param [\"RN50\", \"ViT-B/32\", \"ViT-B/16\", \"RN50x4\", \"RN50x16\"] {allow-input: true}\n",
        "clip_model_name = \"ViT-B/32\"\n",
        "\n",
        "#@markdown **I have not experimented with transforms much; these are merely here to showcase how to use them.**\n",
        "random_affine = False  #@param{type: \"boolean\"}\n",
        "random_motion_blur = False  #@param{type: \"boolean\"}\n",
        "random_horizontal_flip = False  #@param{type: \"boolean\"}\n",
        "\n",
        "augs = []\n",
        "if random_affine:\n",
        "    augs.append(kaugs.RandomAffine(degrees=0, translate=(\n",
        "        0.1, 0.1), scale=(0.9, 1.1), shear=0.1))\n",
        "if random_motion_blur:\n",
        "    augs.append(kaugs.RandomMotionBlur(\n",
        "        kernel_size=(1, 5), angle=15, direction=0.5))\n",
        "if random_horizontal_flip:\n",
        "    augs.append(kaugs.RandomHorizontalFlip(p=0.5))\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "#@title Run\n",
        "clear_scrollback = False  # @param {type:\"boolean\"}\n",
        "\n",
        "assert 0 < save_frequency <= int(timestep_respacing.replace('ddim', '')), \\\n",
        "    \"--save_frequency/--freq must be greater than 0and less than --timestep_respacing\"\n",
        "\n",
        "# convert Path arg to Path object\n",
        "prefix_path = Path('./outputs')\n",
        "prefix_path.mkdir(exist_ok=True)\n",
        "assert prefix_path.is_dir(\n",
        "), f\"--prefix,-dir {prefix_path} is a file, not a directory. Please provide a directory.\"\n",
        "\n",
        "# Initialize diffusion generator\n",
        "cgd_samples, _, diffusion = clip_guided_diffusion(\n",
        "    prompt=prompt, prompt_min=prompt_min, min_weight=min_weight, batch_size=batch_size, tv_scale=tv_scale, top_n=top_n,\n",
        "    image_size=image_size, class_cond=class_cond, clip_guidance_scale=clip_guidance_scale, cutout_power=cutout_power,\n",
        "    num_cutouts=num_cutouts, timestep_respacing=timestep_respacing, custom_device='cuda' if th.cuda.is_available() else 'cpu',\n",
        "    seed=seed, diffusion_steps=diffusion_steps, skip_timesteps=skip_timesteps, init_image=init_image, checkpoints_dir=checkpoints_dir,\n",
        "    clip_model_name=clip_model_name, class_score=class_score,\n",
        ")\n",
        "\n",
        "\n",
        "# Remove non-alphanumeric and white space characters from prompt and prompt_min for directory name\n",
        "outputs_path = cgd_util.txt_to_dir(\n",
        "    base_path=prefix_path, txt=prompt, txt_min=prompt_min)\n",
        "outputs_path.mkdir(exist_ok=True)\n",
        "\n",
        "all_images = []\n",
        "try:\n",
        "    current_timestep = diffusion.num_timesteps - 1\n",
        "    for step, sample in enumerate(cgd_samples):\n",
        "        current_timestep -= 1\n",
        "        if step % save_frequency == 0 or current_timestep == -1:\n",
        "            for j, image in enumerate(sample[\"pred_xstart\"]):\n",
        "                image_path = Path(cgd_util.log_image(\n",
        "                    image, prefix_path, step, j))\n",
        "                ipy_display.display(ipy_display.Image(\"current.png\"))\n",
        "                all_images.append(image_path)\n",
        "            if clear_scrollback:\n",
        "                ipy_display.clear_output()\n",
        "except RuntimeError as runtime_ex:\n",
        "    if \"CUDA out of memory\" in str(runtime_ex):\n",
        "        tqdm.write(f\"CUDA OOM error occurred.\")\n",
        "        tqdm.write(\n",
        "            f\"Try lowering --image_size/-size, --batch_size/-bs, --num_cutouts/-cutn\")\n",
        "        tqdm.write(\n",
        "            f\"--clip_model/-clip (currently '{clip_model_name}') can have a large impact on VRAM usage.\")\n",
        "        tqdm.write(\n",
        "            f\"RN50 will use the least VRAM. ViT-B/32 is the best bang for your buck.\")\n",
        "        tqdm.write(\n",
        "            f\"(colab) Restart the runtime. This is required upon rerunning cells due to a memory leak.\")\n",
        "        # Seems to be a memory leak specific to ipython/jupyter/pytorch\n",
        "        # If you're rerunning this cell, you may need to restart the runtime.\n",
        "    else:\n",
        "        raise runtime_ex\n",
        "\n",
        "print(\n",
        "    f\"Saved {len(all_images)} total images from guided diffusion to {outputs_path})\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://openaipublic.blob.core.windows.net/diffusion/jul-2021/128x128_diffusion.pt to checkpoints/128x128_diffusion.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/home/samsepiol/.cache/clip-guided-diffusion/tmp_checkpoints/128x128_diffusion.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_791977/1901688085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Initialize diffusion generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m cgd_samples, _, diffusion = clip_guided_diffusion(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtv_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_cond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_cond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_guidance_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip_guidance_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutout_power\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcutout_power\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/clip-guided-diffusion/cgd.py\u001b[0m in \u001b[0;36mclip_guided_diffusion\u001b[0;34m(prompt, prompt_min, min_weight, batch_size, tv_scale, top_n, image_size, class_cond, clip_guidance_scale, cutout_power, num_cutouts, timestep_respacing, custom_device, seed, diffusion_steps, skip_timesteps, init_image, checkpoints_dir, clip_model_name, class_score, augs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Download pretrained Guided Diffusion model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mcheckpoints_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoints_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mdiffusion_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcgd_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_guided_diffusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoints_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoints_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_cond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Load CLIP model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/clip-guided-diffusion/cgd_util.py\u001b[0m in \u001b[0;36mdownload_guided_diffusion\u001b[0;34m(image_size, checkpoints_dir, class_cond, overwrite)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Downloading {gd_url} to {gd_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgd_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgd_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgd_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/clip-guided-diffusion/cgd_util.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, filename, root)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mdownload_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mdownload_target_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'tmp_{filename}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mdownload_target_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_target\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         raise RuntimeError(\n",
            "\u001b[0;32m~/.miniconda3/lib/python3.9/pathlib.py\u001b[0m in \u001b[0;36mmkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \"\"\"\n\u001b[1;32m   1311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparents\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/home/samsepiol/.cache/clip-guided-diffusion/tmp_checkpoints/128x128_diffusion.pt'"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}